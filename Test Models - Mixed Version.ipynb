{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hatespeech import preprocessing\n",
    "from hatespeech import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Embedding, Flatten, Dense, Dropout, LSTM, Conv1D, MaxPooling1D, Concatenate, Input\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from gensim.models import FastText\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'Data/Datasets/train_data.csv'\n",
    "dev_path = 'Data/Datasets/dev_data.csv'\n",
    "test_path = 'Data/Datasets/test_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, labels, cnt = preprocessing.load_datasets(train_path, dev_path, test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen_ngrams = 100\n",
    "sequences, word_index, mfws, max_words_ngrams = preprocessing.tokenize_texts_ngrams(texts, ngrams = True, chars = 4)\n",
    "data_reshaped, labels_reshaped = preprocessing.reshape(sequences, labels, maxlen = maxlen_ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen_chars = 280\n",
    "max_words_chars = 670\n",
    "texts_chars = preprocessing.tokenize_texts_characters(texts)\n",
    "data_reshaped_chars, labels_reshaped_chars, word_index_chars = preprocessing.reshape_characters(texts_chars, labels, maxlen = maxlen_chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Datasets and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data_reshaped[:12000]\n",
    "y_train = labels_reshaped[:12000]\n",
    "y_train = to_categorical(y_train)\n",
    "x_dev = data_reshaped[12000:15000]\n",
    "y_dev = labels_reshaped[12000:15000]\n",
    "y_dev = to_categorical(y_dev)\n",
    "x_test = data_reshaped[15000:18000]\n",
    "y_test = labels_reshaped[15000:18000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_chars = data_reshaped_chars[:12000]\n",
    "\n",
    "x_dev_chars = data_reshaped_chars[12000:15000]\n",
    "\n",
    "x_test_chars = data_reshaped_chars[15000:18000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(path, word_index, embdding_dim=300, save=False):\n",
    "    \n",
    "    vectors = FastText.load_fasttext_format(path, encoding='utf-8')\n",
    "  \n",
    "    embedding_matrix=np.zeros((max_words, embedding_dim))\n",
    "    for word, i in word_index.items(): \n",
    "        if i < max_words:\n",
    "            try:\n",
    "                embedding_vector=vectors.wv[word]\n",
    "            except KeyError:\n",
    "           \n",
    "                print(word, 'ist nicht enthalten.')\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "    if save==True:            \n",
    "        pickle.dump( embedding_matrix, open(\"embeddings_ngrams_small.p\", \"wb\" ) )\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_f1_scores(f1_array, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        for score in f1_array:\n",
    "            f.write(\"%s\\n\" % score)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"Average: %s\"  % np.mean(f1_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: These Embeddings were created once via FastText with the method above, then saved. That is why the Embeddings are loaded via pickle in this case. They could also be created again with the method above, should the pickled version not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to FastText Embeddings\n",
    "path = r'G:\\Fasttext\\cc.en.300.bin\\\\cc.en.300.bin'\n",
    "embedding_matrix_ngrams = pickle.load(open( \"embeddings_ngrams_small.p\", \"rb\" ))\n",
    "#embedding_matrix = create_embedding_matrix(path, word_index=word_index, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mixed_model(max_words_ngrams, maxlen_ngrams, max_words_chars, maxlen_chars, embedding_matrix_ngrams, embedding_matrix_chars, embedding_dim=300):\n",
    "    \n",
    "    ngram_input= Input(shape=(100,), name='ngrams')\n",
    "    ngram_embeds = Embedding(max_words_ngrams, embedding_dim, input_length=maxlen_ngrams)(ngram_input)\n",
    "    \n",
    "    x = LSTM(5)(ngram_embeds)\n",
    "    \n",
    "    char_input= Input(shape=(280,), name='characters')\n",
    "    char_embeds = Embedding(max_words_chars, embedding_dim, input_length=maxlen_chars)(char_input)\n",
    "    \n",
    "    \n",
    "    y = Conv1D(16, 5, activation='relu')(char_embeds)\n",
    "    y = MaxPooling1D(3)(y)\n",
    "    y = Conv1D(32, 5, activation='relu')(y)\n",
    "    y = MaxPooling1D(3)(y)\n",
    "    y = Conv1D(64, 5, activation='relu')(y)\n",
    "    y = MaxPooling1D(3)(y)\n",
    "    y = Flatten()(y)\n",
    "    y = Dense(5, activation='relu')(y)\n",
    "    \n",
    "    \n",
    "    concat = Concatenate()([x, y])\n",
    "    z=layers.Dropout(0.5)(concat)\n",
    "    output_tensor=layers.Dense(3, activation='softmax')(z)\n",
    "    \n",
    "    model = Model([ngram_input, char_input], output_tensor)\n",
    "    \n",
    "    \n",
    "    model.layers[9].set_weights([embedding_matrix_ngrams]) \n",
    "    model.layers[9].trainable = False \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(x_train, y_train, x_dev, y_dev, x_train_chars, x_dev_chars, model):\n",
    "    \"\"\"\n",
    "    Fits a model on a given train set (data and labels). Returns model and history. (Rewritten two fit two inputs.)\n",
    "    \"\"\"\n",
    "\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    \n",
    "    history=model.fit({'ngrams': x_train, 'characters': x_train_chars},\n",
    "                     y_train, \n",
    "                     epochs=15,\n",
    "                     batch_size=32,\n",
    "                     validation_data=({'ngrams': x_dev, 'characters': x_dev_chars}, y_dev))\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_predictions(model, x_test, x_test_chars):\n",
    "    \"\"\"\n",
    "    Gets predictions given test data (array) and a model. Returns array of predictions. (Rewritten to fit two inputs.)\n",
    "    \"\"\"\n",
    "    predictions = model.predict({'ngrams': x_test, 'characters': x_test_chars})\n",
    "    y_pred = []\n",
    "    for pred in predictions:\n",
    "        pred = list(pred)\n",
    "        y_pred.append(pred.index(max(pred)))\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f1_array = []\n",
    "i = 0\n",
    "cm = np.zeros(shape=(3,3))\n",
    "\n",
    "while i < 20:\n",
    "    print(i)\n",
    " \n",
    "    model = create_mixed_model(max_words_ngrams, maxlen_ngrams, max_words_chars, maxlen_chars, embedding_matrix_ngrams, embedding_matrix_chars=None, embedding_dim=300)\n",
    "    model, history = fit_model(x_train, y_train, x_dev, y_dev, x_train_chars, x_dev_chars, model)\n",
    "    \n",
    "    predictions = get_test_predictions(model, x_test, x_test_chars)\n",
    "    f1 = evaluation.print_f1_scores(y_test, predictions)\n",
    "    f1_array.append(f1)\n",
    "    \n",
    "    cnf_matrix = confusion_matrix(y_test, predictions)    \n",
    "    cm = cm+cnf_matrix\n",
    "    i +=1\n",
    "    \n",
    "\n",
    "evaluation.plot_confusion_matrix(cm, classes=['Hassrede', 'Beleidigung', 'Neutral'], normalize=True,\n",
    "                      title=' ')\n",
    "#save_f1_scores(f1_array, 'mixed_ver_results.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
