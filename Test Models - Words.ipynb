{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hatespeech import preprocessing\n",
    "from hatespeech import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, Dropout, LSTM, Conv1D, MaxPooling1D, Add\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras import Input, layers, Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from gensim.models import FastText\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'Data/Datasets/train_data.csv'\n",
    "dev_path = 'Data/Datasets/dev_data.csv'\n",
    "test_path = 'Data/Datasets/test_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, labels, cnt = preprocessing.load_datasets(train_path, dev_path, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 100\n",
    "sequences, word_index, mfws, max_words = preprocessing.tokenize_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reshaped, labels_reshaped = preprocessing.reshape(sequences, labels, maxlen = maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Datasets and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data_reshaped[:12000]\n",
    "y_train = labels_reshaped[:12000]\n",
    "x_dev = data_reshaped[12000:15000]\n",
    "y_dev = labels_reshaped[12000:15000]\n",
    "x_test = data_reshaped[15000:18000]\n",
    "y_test = labels_reshaped[15000:18000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 300\n",
    "def create_embedding_matrix(path, word_index, embdding_dim=300, save=False, save_as=\"embeddings.p\"):\n",
    "    \n",
    "    vectors = FastText.load_fasttext_format(path, encoding='utf-8')\n",
    "  \n",
    "    embedding_matrix=np.zeros((max_words, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i < max_words:\n",
    "            try:\n",
    "                embedding_vector=vectors.wv[word]\n",
    "            except KeyError:\n",
    "           \n",
    "                print(word, 'ist nicht enthalten.')\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "    if save==True:            \n",
    "        pickle.dump( embedding_matrix, open(save_as, \"wb\" ) )\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: These Embeddings were created once via FastText with the method above, then saved. That is why the Embeddings are loaded via pickle in this case. They could also be created again with the method above, should the pickled version not work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path to FastText Embeddings\n",
    "path = r'G:\\Fasttext\\cc.en.300.bin\\\\cc.en.300.bin'\n",
    "#embedding_matrix = create_embedding_matrix(path, word_index=word_index, save=True, save_as=\"embeddings_words_small.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = pickle.load(open(\"embeddings_words_small.p\", \"rb\" ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model LSTM\n",
    "def create_LSTM_model(maxlen, max_words, embedding_dim, embedding_matrix):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "    model.add(LSTM(5))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "\n",
    "    model.layers[0].set_weights([embedding_matrix]) \n",
    "    model.layers[0].trainable = False \n",
    "\n",
    "    #model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model CNN_LSTM\n",
    "def create_CNN_LSTM_model(maxlen, max_words, embedding_dim, embedding_matrix):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "    model.add(Conv1D(32, 5, activation='relu'))\n",
    "    model.add(MaxPooling1D(3))\n",
    "    model.add(LSTM(5))\n",
    "    model.add(Dropout(0.5))\n",
    "\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "\n",
    "    model.layers[0].set_weights([embedding_matrix]) \n",
    "    model.layers[0].trainable = False \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model Dense\n",
    "def create_Dense_model(maxlen, max_words, embedding_dim, embedding_matrix):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(5, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "\n",
    "    model.layers[0].set_weights([embedding_matrix]) \n",
    "    model.layers[0].trainable = False \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Model BiLSTM\n",
    "def create_BiLSTM_model(max_words, maxlen, embedding_dim, embedding_matrix):\n",
    "    input_tensor= Input(shape=(100,))\n",
    "    x = Embedding(max_words, embedding_dim, input_length=maxlen)(input_tensor)\n",
    "    left = LSTM(5)(x)\n",
    "    right = LSTM(5, go_backwards=True)(x)\n",
    "    added = Add()([left, right])\n",
    "    z=layers.Dropout(0.5)(added)\n",
    "    output_tensor=layers.Dense(3, activation='softmax')(z)\n",
    "\n",
    "    model = Model(input_tensor, output_tensor)\n",
    "    model.layers[1].set_weights([embedding_matrix]) \n",
    "    model.layers[1].trainable = False \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(x_train, y_train, x_dev, y_dev, model):\n",
    "    \"\"\"\n",
    "    Fits a model on a given train set (data and labels). Returns model and history.\n",
    "    \"\"\"\n",
    "    cat_y_train = to_categorical(y_train)\n",
    "    cat_y_dev = to_categorical(y_dev)\n",
    "\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    history=model.fit(x_train, cat_y_train,\n",
    "                     epochs=15,\n",
    "                     batch_size=32,\n",
    "                     validation_data=(x_dev, cat_y_dev))\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_f1_scores(f1_array, output_file):\n",
    "    with open(output_file, 'w') as f:\n",
    "        for score in f1_array:\n",
    "            f.write(\"%s\\n\" % score)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"Average: %s\"  % np.mean(f1_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f1_array = []\n",
    "i = 0\n",
    "cm = np.zeros(shape=(3,3))\n",
    "while i < 20:\n",
    "    print(i)\n",
    "   \n",
    "    model = create_LSTM_model(maxlen=maxlen, max_words=max_words, embedding_dim=embedding_dim, embedding_matrix=embedding_matrix)\n",
    "    #model.summary()\n",
    "    model, history = fit_model(x_train, y_train, x_dev, y_dev, model)\n",
    "    predictions = evaluation.get_test_predictions(model, x_test)\n",
    "    f1 = evaluation.print_f1_scores(y_test, predictions)\n",
    "    f1_array.append(f1)\n",
    "    \n",
    "    cnf_matrix = confusion_matrix(y_test, predictions)    \n",
    "    cm = cm+cnf_matrix\n",
    "    i +=1\n",
    "\n",
    "print(cm)\n",
    "evaluation.plot_confusion_matrix(cm, classes=['Hassrede', 'Beleidigung', 'Neutral'], normalize=True,\n",
    "                      title=' ')\n",
    "#save_f1_scores(f1_array, 'word_LSTM_results.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_array = []\n",
    "i = 0\n",
    "cm = np.zeros(shape=(3,3))\n",
    "while i < 20:\n",
    "    print(i)\n",
    "    \n",
    "    model = create_Dense_model(maxlen=maxlen, max_words=max_words, embedding_dim=embedding_dim, embedding_matrix=embedding_matrix)\n",
    "    model, history = fit_model(x_train, y_train, x_dev, y_dev, model)\n",
    "    predictions = evaluation.get_test_predictions(model, x_test)\n",
    "    f1 = evaluation.print_f1_scores(y_test, predictions)\n",
    "    f1_array.append(f1)\n",
    "    \n",
    "    cnf_matrix = confusion_matrix(y_test, predictions)    \n",
    "    cm = cm+cnf_matrix\n",
    "    i +=1\n",
    "\n",
    "print(cm)    \n",
    "evaluation.plot_confusion_matrix(cnf_matrix, classes=['Hassrede', 'Beleidigung', 'Neutral'], normalize=True,\n",
    "                      title=' ')\n",
    "#save_f1_scores(f1_array, 'word_Dense_results.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_array = []\n",
    "i = 0\n",
    "cm = np.zeros(shape=(3,3))\n",
    "\n",
    "while i < 20:\n",
    "    print(i)\n",
    "   \n",
    "    model = create_CNN_LSTM_model(maxlen=maxlen, max_words=max_words, embedding_dim=embedding_dim, embedding_matrix=embedding_matrix)\n",
    "    model, history = fit_model(x_train, y_train, x_dev, y_dev, model)\n",
    "    predictions = evaluation.get_test_predictions(model, x_test)\n",
    "    f1 = evaluation.print_f1_scores(y_test, predictions)\n",
    "    f1_array.append(f1)\n",
    "    \n",
    "    cnf_matrix = confusion_matrix(y_test, predictions)    \n",
    "    cm = cm+cnf_matrix\n",
    "    i +=1\n",
    "\n",
    "print(cm)\n",
    "evaluation.plot_confusion_matrix(cnf_matrix, classes=['Hassrede', 'Beleidigung', 'Neutral'], normalize=True,\n",
    "                      title=' ')\n",
    "#save_f1_scores(f1_array, 'word_CNNLSTM_results.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_array = []\n",
    "i = 0\n",
    "cm = np.zeros(shape=(3,3))\n",
    "while i < 20:\n",
    "    print(i)\n",
    "    \n",
    "    model = create_BiLSTM_model(maxlen=maxlen, max_words=max_words, embedding_dim=embedding_dim, embedding_matrix=embedding_matrix)\n",
    "    model, history = fit_model(x_train, y_train, x_dev, y_dev, model)\n",
    "    predictions = evaluation.get_test_predictions(model, x_test)\n",
    "    f1 = evaluation.print_f1_scores(y_test, predictions)\n",
    "    f1_array.append(f1)\n",
    "    \n",
    "    cnf_matrix = confusion_matrix(y_test, predictions)    \n",
    "    cm = cm+cnf_matrix\n",
    "    i +=1\n",
    "      \n",
    "print(cm)\n",
    "evaluation.plot_confusion_matrix(cnf_matrix, classes=['Hassrede', 'Beleidigung', 'Neutral'], normalize=True,\n",
    "                      title=' ')\n",
    "#save_f1_scores(f1_array, 'word_BILSTM_results.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
