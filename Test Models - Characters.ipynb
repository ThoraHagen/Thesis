{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hatespeech import preprocessing\n",
    "from hatespeech import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import Sequential\n",
    "from keras.utils.np_utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'Data/Datasets/train_data.csv'\n",
    "dev_path = 'Data/Datasets/dev_data.csv'\n",
    "test_path = 'Data/Datasets/test_data.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, labels, cnt = preprocessing.load_datasets(train_path, dev_path, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 670"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 280\n",
    "texts = preprocessing.tokenize_texts_characters(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reshaped, labels_reshaped, word_index = preprocessing.reshape_characters(texts, labels, maxlen = maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Datasets and Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data_reshaped[:12000]\n",
    "y_train = labels_reshaped[:12000]\n",
    "y_train = to_categorical(y_train)\n",
    "x_dev = data_reshaped[12000:15000]\n",
    "y_dev = labels_reshaped[12000:15000]\n",
    "y_dev = to_categorical(y_dev)\n",
    "x_test = data_reshaped[15000:18000]\n",
    "y_test = labels_reshaped[15000:18000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare One-Hot Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_gen(texts, labels, n=10):\n",
    "    labels_array = []\n",
    "    result = np.zeros(shape= (n, maxlen, max_words))\n",
    " \n",
    "    while 1:\n",
    "        \n",
    "        i = 0\n",
    "        labels_i = 0\n",
    "        for sample in texts:\n",
    "         \n",
    "            labels_array.append(labels[labels_i])\n",
    "            labels_i += 1\n",
    "            for j, character in enumerate(sample):\n",
    "                index = character\n",
    "            \n",
    "                result[i, j, index] = 1.\n",
    "            i += 1\n",
    "\n",
    "            if len(labels_array) >= n:\n",
    "\n",
    "                labels_array=np.asarray(labels_array)             \n",
    "        \n",
    "                yield result, labels_array\n",
    "                i = 0\n",
    "                result = np.zeros(shape= (n, maxlen, max_words))\n",
    "                labels_array = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(word_index.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(texts):\n",
    "   \n",
    "    results = np.zeros((len(texts), maxlen, max_words))\n",
    "    for i, sample in enumerate(texts):\n",
    "        #sample = sample.lower()\n",
    "        for j, character in enumerate(sample):\n",
    "            \n",
    "            index = character\n",
    "           \n",
    "            results[i, j, index] = 1.\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_x_test = preprocessing.one_hot(x_test, maxlen, max_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oh_x_train_gen = preprocessing.one_hot_gen(x_train, y_train, maxlen, max_words, n = 10)\n",
    "oh_x_dev_gen = preprocessing.one_hot_gen(x_dev, y_dev, maxlen, max_words,n = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Pretrained Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_matrix(path, word_index, max_words, embedding_dim=300):\n",
    "\n",
    "    model = FastText.load(path)\n",
    "    \n",
    "    embedding_matrix=np.zeros((max_words, embedding_dim))\n",
    "    for word, i in word_index.items():\n",
    "        if i < max_words:\n",
    "            try:\n",
    "                embedding_vector=model.wv[word]\n",
    "            except KeyError:\n",
    "                \n",
    "                print(word, 'ist nicht enthalten.')\n",
    "                embedding_vector = [0] * embedding_dim\n",
    "                    \n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                \n",
    "    #pickle.dump(embedding_matrix, open(\"embeddings.p\", \"wb\" ) )\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Originally created in \"Preprocessing/Twitter Character Embeddings\"\n",
    "embedding_matrix = create_embedding_matrix(path='embeddings_chars.model', word_index=word_index, max_words=max_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(maxlen, max_words, embed='pretrained', embedding_matrix=False, embedding_dim =300):\n",
    "    \"\"\"\n",
    "    Creates keras model including embeddings.\n",
    "    \"\"\"\n",
    " \n",
    "    model = Sequential()\n",
    "    if embed == 'pretrained' or embed == 'self-train':\n",
    "        model.add(layers.Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "        model.add(layers.Conv1D(16, 5, activation='relu'))\n",
    "    if embed == 'none':\n",
    "        model.add(layers.Conv1D(16, 5, activation='relu', input_shape=(maxlen, max_words)))\n",
    "\n",
    "        \n",
    "    model.add(layers.MaxPooling1D(3))\n",
    "    model.add(layers.Conv1D(32, 5, activation='relu'))\n",
    "    model.add(layers.MaxPooling1D(3))\n",
    "    model.add(layers.Conv1D(64, 5, activation='relu'))\n",
    "    model.add(layers.MaxPooling1D(3))\n",
    "  \n",
    "\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dropout(0.5))\n",
    "    model.add(layers.Dense(5, activation='relu'))\n",
    "\n",
    "    model.add(layers.Dense(3, activation='softmax'))\n",
    "\n",
    "    if embed == 'pretrained':\n",
    "        model.layers[0].set_weights([embedding_matrix]) \n",
    "        model.layers[0].trainable = False\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model(x_train, y_train, x_dev, y_dev, model):\n",
    "    \"\"\"\n",
    "    Fits a model on a given train set (data and labels). Returns model and history.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    history=model.fit(x_train, y_train, \n",
    "                     epochs=15,\n",
    "                     batch_size=32,\n",
    "                     validation_data=(x_dev, y_dev))\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_model_gen(oh_x_train_gen, oh_x_dev_gen, model):\n",
    "    \"\"\"\n",
    "    Fits a model on a given train and val generator (data and labels). Returns model and history.\n",
    "    \"\"\"\n",
    "   \n",
    "\n",
    "    model.compile(optimizer='rmsprop',\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])\n",
    "    history=model.fit_generator(oh_x_train_gen, \n",
    "                                epochs=15,\n",
    "                                steps_per_epoch=1200,\n",
    "                                \n",
    "                                validation_data=oh_x_dev_gen,\n",
    "                                validation_steps=300)\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_f1_scores(f1_array, output_file):\n",
    "    with open(ouput_file, 'w') as f:\n",
    "        for score in f1_array:\n",
    "            f.write(\"%s\\n\" % score)\n",
    "        f.write(\"\\n\")\n",
    "        f.write(\"Average: %s\"  % np.mean(f1_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_array = []\n",
    "i = 0\n",
    "cm = np.zeros(shape=(3,3))\n",
    "while i < 20:\n",
    "    print(i)\n",
    "    \n",
    "    model = create_model(maxlen, max_words, embed='self-train', embedding_matrix=embedding_matrix)\n",
    "    model, history = fit_model(x_train, y_train, x_dev, y_dev, model)\n",
    "    \n",
    "    predictions = evaluation.get_test_predictions(model, x_test)\n",
    "    f1 = evaluation.print_f1_scores(y_test, predictions)\n",
    "    f1_array.append(f1)\n",
    "    \n",
    "    cnf_matrix = confusion_matrix(y_test, predictions)    \n",
    "    cm = cm+cnf_matrix\n",
    "    i +=1\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test, predictions)    \n",
    "evaluation.plot_confusion_matrix(cnf_matrix, classes=['Hassrede', 'Beleidigung', 'Neutral'], normalize=True,\n",
    "                      title=' ')\n",
    "#save_f1_scores(f1_array, 'char_selftrained_results.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f1_array = []\n",
    "i = 0\n",
    "cm = np.zeros(shape=(3,3))\n",
    "while i < 20:\n",
    "    print(i)\n",
    "    \n",
    "    model = create_model(maxlen, max_words, embed='pretrained', embedding_matrix=embedding_matrix)\n",
    "    model, history = fit_model(x_train, y_train, x_dev, y_dev, model)\n",
    "    \n",
    "    predictions = evaluation.get_test_predictions(model, x_test)\n",
    "    f1 = evaluation.print_f1_scores(y_test, predictions)\n",
    "    f1_array.append(f1)\n",
    "    \n",
    "    cnf_matrix = confusion_matrix(y_test, predictions)    \n",
    "    cm = cm+cnf_matrix\n",
    "    i +=1\n",
    "\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test, predictions)    \n",
    "evaluation.plot_confusion_matrix(cnf_matrix, classes=['Hassrede', 'Beleidigung', 'Neutral'], normalize=True,\n",
    "                      title=' ')\n",
    "#save_f1_scores(f1_array, 'char_pretrained_results.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f1_array = []\n",
    "i = 0\n",
    "cm = np.zeros(shape=(3,3))\n",
    "while i < 20:\n",
    "    print(i)\n",
    "    \n",
    "    model = create_model(maxlen, max_words, embed='none')\n",
    "    model, history = fit_model_gen(oh_x_train_gen, oh_x_dev_gen, model)\n",
    "    \n",
    "    predictions = evaluation.get_test_predictions(model, oh_x_test)\n",
    "    f1 = evaluation.print_f1_scores(y_test, predictions)\n",
    "    f1_array.append(f1)\n",
    "    \n",
    "    cnf_matrix = confusion_matrix(y_test, predictions)    \n",
    "    cm = cm+cnf_matrix\n",
    "    i +=1\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test, predictions)    \n",
    "evaluation.plot_confusion_matrix(cnf_matrix, classes=['Hassrede', 'Beleidigung', 'Neutral'], normalize=True,\n",
    "                      title=' ')   \n",
    "#save_f1_scores(f1_array, 'char_oh_results.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
