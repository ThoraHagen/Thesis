{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\thora\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from gensim.test.utils import get_tmpfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import regex as re\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.layers import Embedding, Flatten, Dense, Dropout, LSTM\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Twitter Character Embeddings from twcs.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('twcs.csv', sep=',', encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts=list(df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "\n",
    "# Function takes a tokenized sentence and returns the words\n",
    "def sequence_to_text(list_of_indices):\n",
    "    # Looking up words in dictionary\n",
    "    words = [reverse_word_map.get(letter) for letter in list_of_indices]\n",
    "    return(words)\n",
    "\n",
    "# Creating texts \n",
    "t = list(map(sequence_to_text, sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "path = get_tmpfile(\"chars_twitter.model\")\n",
    "\n",
    "model = FastText(t, size=300, min_count=1, workers=4)\n",
    "model.save(\"chars_twitter.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tune the Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastText.load(\"chars_twitter.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(csv, p1, p2):\n",
    "    \"\"\"\n",
    "    Loads data from 3 specific paths. \n",
    "    Returns a list of texts, a corresponding list of labels (0|1|2), and a labels counter.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv, index_col=0, encoding = 'utf-8')\n",
    "\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for row in df.itertuples(index=True):\n",
    "        tweet = getattr(row, \"tweet\")\n",
    "        classif = getattr(row, \"classification\")\n",
    "        texts.append(tweet)\n",
    "        labels.append(classif)\n",
    "\n",
    "    more_hateful_tweets = pickle.load(open(p1, \"rb\" ))\n",
    "    more_clean_tweets = pickle.load(open(p2, \"rb\" ))\n",
    "\n",
    "    hateful_labels = [0] * len(more_hateful_tweets)\n",
    "    clean_labels = [2] * len(more_clean_tweets)\n",
    "    print('Additional Hateful:', len(more_hateful_tweets), 'Additional Clean:', len(more_clean_tweets))\n",
    "    \n",
    "    texts = texts+more_hateful_tweets+more_clean_tweets\n",
    "    labels = labels+hateful_labels+clean_labels\n",
    "    cnt = Counter(labels)\n",
    "    print(cnt)\n",
    "    return texts, labels, cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Additional Hateful: 9802 Additional Clean: 3017\n",
      "Counter({1: 19190, 0: 11232, 2: 7180})\n"
     ]
    }
   ],
   "source": [
    "texts, labels, cnt = load_data('../../Data/Data_Original/offensive_language_crowdflower.csv', \"../../Data/Data_Original/tweets_hate_icwsm18_extended.p\", \"../../Data/Data_Original/tweets_clean_zeerakW_extended.p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_texts_characters(corpus):\n",
    "    \"\"\"\n",
    "    Tokenizes a list of texts. ...!\n",
    "    \"\"\"\n",
    "    corpus = convert_html_emojis(corpus)\n",
    "    \n",
    "    tknzr = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "    \n",
    "    \n",
    "    token_doc_list = []\n",
    "    token_list = []\n",
    "   \n",
    "    for text in corpus:\n",
    "     \n",
    "\n",
    "        tokens = tknzr.tokenize(text)\n",
    "        tokens2 = []\n",
    "        for token in tokens:\n",
    "        #    token = re.sub(r'([1234567890!\"#$%&()*+,./:@;?[\\]^`{|}_~\\t\\n])', '', token)\n",
    "            if token != '' and token !='charlie' and token !='yankees':\n",
    "                tokens2.append(token)\n",
    "        \n",
    "\n",
    "        \n",
    "        tmp_tokens = []\n",
    "        for i, token in enumerate(tokens2):\n",
    "            #if token in UNICODE_EMOJI:\n",
    "               \n",
    "                #emoji_one = emoji.demojize(token)\n",
    "                #emoji_one = emoji_one[1:-1]\n",
    "                #emoji_all = re.split('_', emoji_one)\n",
    "\n",
    "                #tokens[i] = emoji_all\n",
    "                #token_list.extend(emoji_all)\n",
    "                #tmp_tokens.extend(emoji_all)\n",
    "            if token == \"i'd\":\n",
    "                token_list.extend(['i', 'would'])\n",
    "                tmp_tokens.extend(['i', 'would'])\n",
    "            else: \n",
    "                token_list.append(token)\n",
    "                tmp_tokens.append(token)\n",
    "\n",
    "            \n",
    "        token_doc_list.append(tmp_tokens)\n",
    "        \n",
    "    final_texts = []\n",
    "    for doc in token_doc_list:\n",
    "        t = \" \".join(doc)\n",
    "        final_texts.append(t)\n",
    "    return final_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import html\n",
    "def convert_html_emojis(corpus):\n",
    "    new_corpus = []\n",
    "    for text in corpus:\n",
    "        text = html.unescape(text)\n",
    "        new_corpus.append(text)\n",
    "    return new_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from emoji import UNICODE_EMOJI\n",
    "texts = tokenize_texts_characters(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n",
    "d = list(map(sequence_to_text, sequences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37602"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['!',\n",
       "  ' ',\n",
       "  '!',\n",
       "  ' ',\n",
       "  '!',\n",
       "  ' ',\n",
       "  'r',\n",
       "  't',\n",
       "  ' ',\n",
       "  ':',\n",
       "  ' ',\n",
       "  'a',\n",
       "  's',\n",
       "  ' ',\n",
       "  'a',\n",
       "  ' ',\n",
       "  'w',\n",
       "  'o',\n",
       "  'm',\n",
       "  'a',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'y',\n",
       "  'o',\n",
       "  'u',\n",
       "  ' ',\n",
       "  's',\n",
       "  'h',\n",
       "  'o',\n",
       "  'u',\n",
       "  'l',\n",
       "  'd',\n",
       "  'n',\n",
       "  \"'\",\n",
       "  't',\n",
       "  ' ',\n",
       "  'c',\n",
       "  'o',\n",
       "  'm',\n",
       "  'p',\n",
       "  'l',\n",
       "  'a',\n",
       "  'i',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'a',\n",
       "  'b',\n",
       "  'o',\n",
       "  'u',\n",
       "  't',\n",
       "  ' ',\n",
       "  'c',\n",
       "  'l',\n",
       "  'e',\n",
       "  'a',\n",
       "  'n',\n",
       "  'i',\n",
       "  'n',\n",
       "  'g',\n",
       "  ' ',\n",
       "  'u',\n",
       "  'p',\n",
       "  ' ',\n",
       "  'y',\n",
       "  'o',\n",
       "  'u',\n",
       "  'r',\n",
       "  ' ',\n",
       "  'h',\n",
       "  'o',\n",
       "  'u',\n",
       "  's',\n",
       "  'e',\n",
       "  ' ',\n",
       "  '.',\n",
       "  ' ',\n",
       "  '&',\n",
       "  ' ',\n",
       "  'a',\n",
       "  's',\n",
       "  ' ',\n",
       "  'a',\n",
       "  ' ',\n",
       "  'm',\n",
       "  'a',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'y',\n",
       "  'o',\n",
       "  'u',\n",
       "  ' ',\n",
       "  's',\n",
       "  'h',\n",
       "  'o',\n",
       "  'u',\n",
       "  'l',\n",
       "  'd',\n",
       "  ' ',\n",
       "  'a',\n",
       "  'l',\n",
       "  'w',\n",
       "  'a',\n",
       "  'y',\n",
       "  's',\n",
       "  ' ',\n",
       "  't',\n",
       "  'a',\n",
       "  'k',\n",
       "  'e',\n",
       "  ' ',\n",
       "  't',\n",
       "  'h',\n",
       "  'e',\n",
       "  ' ',\n",
       "  't',\n",
       "  'r',\n",
       "  'a',\n",
       "  's',\n",
       "  'h',\n",
       "  ' ',\n",
       "  'o',\n",
       "  'u',\n",
       "  't',\n",
       "  ' ',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.'],\n",
       " ['!',\n",
       "  ' ',\n",
       "  '!',\n",
       "  ' ',\n",
       "  '!',\n",
       "  ' ',\n",
       "  'r',\n",
       "  't',\n",
       "  ' ',\n",
       "  ':',\n",
       "  ' ',\n",
       "  'b',\n",
       "  'o',\n",
       "  'y',\n",
       "  ' ',\n",
       "  'd',\n",
       "  'a',\n",
       "  't',\n",
       "  's',\n",
       "  ' ',\n",
       "  'c',\n",
       "  'o',\n",
       "  'l',\n",
       "  'd',\n",
       "  ' ',\n",
       "  '.',\n",
       "  '.',\n",
       "  '.',\n",
       "  ' ',\n",
       "  't',\n",
       "  'y',\n",
       "  'g',\n",
       "  'a',\n",
       "  ' ',\n",
       "  'd',\n",
       "  'w',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'b',\n",
       "  'a',\n",
       "  'd',\n",
       "  ' ',\n",
       "  'f',\n",
       "  'o',\n",
       "  'r',\n",
       "  ' ',\n",
       "  'c',\n",
       "  'u',\n",
       "  'f',\n",
       "  'f',\n",
       "  'i',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'd',\n",
       "  'a',\n",
       "  't',\n",
       "  ' ',\n",
       "  'h',\n",
       "  'o',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'i',\n",
       "  'n',\n",
       "  ' ',\n",
       "  't',\n",
       "  'h',\n",
       "  'e',\n",
       "  ' ',\n",
       "  '1',\n",
       "  's',\n",
       "  't',\n",
       "  ' ',\n",
       "  'p',\n",
       "  'l',\n",
       "  'a',\n",
       "  'c',\n",
       "  'e',\n",
       "  ' ',\n",
       "  '!',\n",
       "  ' ',\n",
       "  '!'],\n",
       " ['!',\n",
       "  ' ',\n",
       "  '!',\n",
       "  ' ',\n",
       "  '!',\n",
       "  ' ',\n",
       "  'r',\n",
       "  't',\n",
       "  ' ',\n",
       "  'd',\n",
       "  'a',\n",
       "  'w',\n",
       "  'g',\n",
       "  ' ',\n",
       "  '!',\n",
       "  ' ',\n",
       "  '!',\n",
       "  ' ',\n",
       "  '!',\n",
       "  ' ',\n",
       "  'r',\n",
       "  't',\n",
       "  ' ',\n",
       "  ':',\n",
       "  ' ',\n",
       "  'y',\n",
       "  'o',\n",
       "  'u',\n",
       "  ' ',\n",
       "  'e',\n",
       "  'v',\n",
       "  'e',\n",
       "  'r',\n",
       "  ' ',\n",
       "  'f',\n",
       "  'u',\n",
       "  'c',\n",
       "  'k',\n",
       "  ' ',\n",
       "  'a',\n",
       "  ' ',\n",
       "  'b',\n",
       "  'i',\n",
       "  't',\n",
       "  'c',\n",
       "  'h',\n",
       "  ' ',\n",
       "  'a',\n",
       "  'n',\n",
       "  'd',\n",
       "  ' ',\n",
       "  's',\n",
       "  'h',\n",
       "  'e',\n",
       "  ' ',\n",
       "  's',\n",
       "  't',\n",
       "  'a',\n",
       "  'r',\n",
       "  't',\n",
       "  ' ',\n",
       "  't',\n",
       "  'o',\n",
       "  ' ',\n",
       "  'c',\n",
       "  'r',\n",
       "  'y',\n",
       "  ' ',\n",
       "  '?',\n",
       "  ' ',\n",
       "  'y',\n",
       "  'o',\n",
       "  'u',\n",
       "  ' ',\n",
       "  'b',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'c',\n",
       "  'o',\n",
       "  'n',\n",
       "  'f',\n",
       "  'u',\n",
       "  's',\n",
       "  'e',\n",
       "  'd',\n",
       "  ' ',\n",
       "  'a',\n",
       "  's',\n",
       "  ' ',\n",
       "  's',\n",
       "  'h',\n",
       "  'i',\n",
       "  't'],\n",
       " ['!',\n",
       "  ' ',\n",
       "  '!',\n",
       "  ' ',\n",
       "  '!',\n",
       "  ' ',\n",
       "  'r',\n",
       "  't',\n",
       "  ' ',\n",
       "  ':',\n",
       "  ' ',\n",
       "  's',\n",
       "  'h',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'l',\n",
       "  'o',\n",
       "  'o',\n",
       "  'k',\n",
       "  ' ',\n",
       "  'l',\n",
       "  'i',\n",
       "  'k',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'a',\n",
       "  ' ',\n",
       "  't',\n",
       "  'r',\n",
       "  'a',\n",
       "  'n',\n",
       "  'n',\n",
       "  'y'],\n",
       " ['!',\n",
       "  ' ',\n",
       "  '!',\n",
       "  ' ',\n",
       "  '!',\n",
       "  ' ',\n",
       "  'r',\n",
       "  't',\n",
       "  ' ',\n",
       "  ':',\n",
       "  ' ',\n",
       "  't',\n",
       "  'h',\n",
       "  'e',\n",
       "  ' ',\n",
       "  's',\n",
       "  'h',\n",
       "  'i',\n",
       "  't',\n",
       "  ' ',\n",
       "  'y',\n",
       "  'o',\n",
       "  'u',\n",
       "  ' ',\n",
       "  'h',\n",
       "  'e',\n",
       "  'a',\n",
       "  'r',\n",
       "  ' ',\n",
       "  'a',\n",
       "  'b',\n",
       "  'o',\n",
       "  'u',\n",
       "  't',\n",
       "  ' ',\n",
       "  'm',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'm',\n",
       "  'i',\n",
       "  'g',\n",
       "  'h',\n",
       "  't',\n",
       "  ' ',\n",
       "  'b',\n",
       "  'e',\n",
       "  ' ',\n",
       "  't',\n",
       "  'r',\n",
       "  'u',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'o',\n",
       "  'r',\n",
       "  ' ',\n",
       "  'i',\n",
       "  't',\n",
       "  ' ',\n",
       "  'm',\n",
       "  'i',\n",
       "  'g',\n",
       "  'h',\n",
       "  't',\n",
       "  ' ',\n",
       "  'b',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'f',\n",
       "  'a',\n",
       "  'k',\n",
       "  'e',\n",
       "  'r',\n",
       "  ' ',\n",
       "  't',\n",
       "  'h',\n",
       "  'a',\n",
       "  'n',\n",
       "  ' ',\n",
       "  't',\n",
       "  'h',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'b',\n",
       "  'i',\n",
       "  't',\n",
       "  'c',\n",
       "  'h',\n",
       "  ' ',\n",
       "  'w',\n",
       "  'h',\n",
       "  'o',\n",
       "  ' ',\n",
       "  't',\n",
       "  'o',\n",
       "  'l',\n",
       "  'd',\n",
       "  ' ',\n",
       "  'i',\n",
       "  't',\n",
       "  ' ',\n",
       "  't',\n",
       "  'o',\n",
       "  ' ',\n",
       "  'y',\n",
       "  'a',\n",
       "  ' ',\n",
       "  '\\ue011'],\n",
       " ['!',\n",
       "  ' ',\n",
       "  '!',\n",
       "  ' ',\n",
       "  '!',\n",
       "  ' ',\n",
       "  '\"',\n",
       "  ' ',\n",
       "  ':',\n",
       "  ' ',\n",
       "  't',\n",
       "  'h',\n",
       "  'e',\n",
       "  ' ',\n",
       "  's',\n",
       "  'h',\n",
       "  'i',\n",
       "  't',\n",
       "  ' ',\n",
       "  'j',\n",
       "  'u',\n",
       "  's',\n",
       "  't',\n",
       "  ' ',\n",
       "  'b',\n",
       "  'l',\n",
       "  'o',\n",
       "  'w',\n",
       "  's',\n",
       "  ' ',\n",
       "  'm',\n",
       "  'e',\n",
       "  ' ',\n",
       "  '.',\n",
       "  '.',\n",
       "  ' ',\n",
       "  'c',\n",
       "  'l',\n",
       "  'a',\n",
       "  'i',\n",
       "  'm',\n",
       "  ' ',\n",
       "  'y',\n",
       "  'o',\n",
       "  'u',\n",
       "  ' ',\n",
       "  's',\n",
       "  'o',\n",
       "  ' ',\n",
       "  'f',\n",
       "  'a',\n",
       "  'i',\n",
       "  't',\n",
       "  'h',\n",
       "  'f',\n",
       "  'u',\n",
       "  'l',\n",
       "  ' ',\n",
       "  'a',\n",
       "  'n',\n",
       "  'd',\n",
       "  ' ',\n",
       "  'd',\n",
       "  'o',\n",
       "  'w',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'f',\n",
       "  'o',\n",
       "  'r',\n",
       "  ' ',\n",
       "  's',\n",
       "  'o',\n",
       "  'm',\n",
       "  'e',\n",
       "  'b',\n",
       "  'o',\n",
       "  'd',\n",
       "  'y',\n",
       "  ' ',\n",
       "  'b',\n",
       "  'u',\n",
       "  't',\n",
       "  ' ',\n",
       "  's',\n",
       "  't',\n",
       "  'i',\n",
       "  'l',\n",
       "  'l',\n",
       "  ' ',\n",
       "  'f',\n",
       "  'u',\n",
       "  'c',\n",
       "  'k',\n",
       "  'i',\n",
       "  'n',\n",
       "  'g',\n",
       "  ' ',\n",
       "  'w',\n",
       "  'i',\n",
       "  't',\n",
       "  'h',\n",
       "  ' ',\n",
       "  'h',\n",
       "  'o',\n",
       "  'e',\n",
       "  's',\n",
       "  ' ',\n",
       "  '!',\n",
       "  ' ',\n",
       "  '😂',\n",
       "  ' ',\n",
       "  '😂',\n",
       "  ' ',\n",
       "  '😂',\n",
       "  ' ',\n",
       "  '\"'],\n",
       " ['!',\n",
       "  ' ',\n",
       "  '!',\n",
       "  ' ',\n",
       "  '!',\n",
       "  ' ',\n",
       "  '\"',\n",
       "  ' ',\n",
       "  ':',\n",
       "  ' ',\n",
       "  'i',\n",
       "  ' ',\n",
       "  'c',\n",
       "  'a',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'n',\n",
       "  'o',\n",
       "  't',\n",
       "  ' ',\n",
       "  'j',\n",
       "  'u',\n",
       "  's',\n",
       "  't',\n",
       "  ' ',\n",
       "  's',\n",
       "  'i',\n",
       "  't',\n",
       "  ' ',\n",
       "  'u',\n",
       "  'p',\n",
       "  ' ',\n",
       "  'a',\n",
       "  'n',\n",
       "  'd',\n",
       "  ' ',\n",
       "  'h',\n",
       "  'a',\n",
       "  't',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'o',\n",
       "  'n',\n",
       "  ' ',\n",
       "  'a',\n",
       "  'n',\n",
       "  'o',\n",
       "  't',\n",
       "  'h',\n",
       "  'e',\n",
       "  'r',\n",
       "  ' ',\n",
       "  'b',\n",
       "  'i',\n",
       "  't',\n",
       "  'c',\n",
       "  'h',\n",
       "  ' ',\n",
       "  '.',\n",
       "  '.',\n",
       "  ' ',\n",
       "  'i',\n",
       "  ' ',\n",
       "  'g',\n",
       "  'o',\n",
       "  't',\n",
       "  ' ',\n",
       "  't',\n",
       "  'o',\n",
       "  'o',\n",
       "  ' ',\n",
       "  'm',\n",
       "  'u',\n",
       "  'c',\n",
       "  'h',\n",
       "  ' ',\n",
       "  's',\n",
       "  'h',\n",
       "  'i',\n",
       "  't',\n",
       "  ' ',\n",
       "  'g',\n",
       "  'o',\n",
       "  'i',\n",
       "  'n',\n",
       "  'g',\n",
       "  ' ',\n",
       "  'o',\n",
       "  'n',\n",
       "  ' ',\n",
       "  '!',\n",
       "  ' ',\n",
       "  '\"'],\n",
       " ['!',\n",
       "  ' ',\n",
       "  '!',\n",
       "  ' ',\n",
       "  '!',\n",
       "  ' ',\n",
       "  '“',\n",
       "  ' ',\n",
       "  ':',\n",
       "  ' ',\n",
       "  'c',\n",
       "  'a',\n",
       "  'u',\n",
       "  's',\n",
       "  'e',\n",
       "  ' ',\n",
       "  'i',\n",
       "  \"'\",\n",
       "  'm',\n",
       "  ' ',\n",
       "  't',\n",
       "  'i',\n",
       "  'r',\n",
       "  'e',\n",
       "  'd',\n",
       "  ' ',\n",
       "  'o',\n",
       "  'f',\n",
       "  ' ',\n",
       "  'y',\n",
       "  'o',\n",
       "  'u',\n",
       "  ' ',\n",
       "  'b',\n",
       "  'i',\n",
       "  'g',\n",
       "  ' ',\n",
       "  'b',\n",
       "  'i',\n",
       "  't',\n",
       "  'c',\n",
       "  'h',\n",
       "  'e',\n",
       "  's',\n",
       "  ' ',\n",
       "  'c',\n",
       "  'o',\n",
       "  'm',\n",
       "  'i',\n",
       "  'n',\n",
       "  'g',\n",
       "  ' ',\n",
       "  'f',\n",
       "  'o',\n",
       "  'r',\n",
       "  ' ',\n",
       "  'u',\n",
       "  's',\n",
       "  ' ',\n",
       "  's',\n",
       "  'k',\n",
       "  'i',\n",
       "  'n',\n",
       "  'n',\n",
       "  'y',\n",
       "  ' ',\n",
       "  'g',\n",
       "  'i',\n",
       "  'r',\n",
       "  'l',\n",
       "  's',\n",
       "  ' ',\n",
       "  '!',\n",
       "  ' ',\n",
       "  '!',\n",
       "  ' ',\n",
       "  '”'],\n",
       " ['\"',\n",
       "  ' ',\n",
       "  '&',\n",
       "  ' ',\n",
       "  'y',\n",
       "  'o',\n",
       "  'u',\n",
       "  ' ',\n",
       "  'm',\n",
       "  'i',\n",
       "  'g',\n",
       "  'h',\n",
       "  't',\n",
       "  ' ',\n",
       "  'n',\n",
       "  'o',\n",
       "  't',\n",
       "  ' ',\n",
       "  'g',\n",
       "  'e',\n",
       "  't',\n",
       "  ' ',\n",
       "  'y',\n",
       "  'a',\n",
       "  ' ',\n",
       "  'b',\n",
       "  'i',\n",
       "  't',\n",
       "  'c',\n",
       "  'h',\n",
       "  ' ',\n",
       "  'b',\n",
       "  'a',\n",
       "  'c',\n",
       "  'k',\n",
       "  ' ',\n",
       "  '&',\n",
       "  ' ',\n",
       "  't',\n",
       "  'h',\n",
       "  'a',\n",
       "  't',\n",
       "  's',\n",
       "  ' ',\n",
       "  't',\n",
       "  'h',\n",
       "  'a',\n",
       "  't',\n",
       "  ' ',\n",
       "  '\"'],\n",
       " ['\"',\n",
       "  ' ',\n",
       "  ':',\n",
       "  ' ',\n",
       "  'h',\n",
       "  'o',\n",
       "  'b',\n",
       "  'b',\n",
       "  'i',\n",
       "  'e',\n",
       "  's',\n",
       "  ' ',\n",
       "  'i',\n",
       "  'n',\n",
       "  'c',\n",
       "  'l',\n",
       "  'u',\n",
       "  'd',\n",
       "  'e',\n",
       "  ' ',\n",
       "  ':',\n",
       "  ' ',\n",
       "  'f',\n",
       "  'i',\n",
       "  'g',\n",
       "  'h',\n",
       "  't',\n",
       "  'i',\n",
       "  'n',\n",
       "  'g',\n",
       "  ' ',\n",
       "  'm',\n",
       "  'a',\n",
       "  'r',\n",
       "  'i',\n",
       "  'a',\n",
       "  'm',\n",
       "  ' ',\n",
       "  '\"',\n",
       "  ' ',\n",
       "  'b',\n",
       "  'i',\n",
       "  't',\n",
       "  'c',\n",
       "  'h']]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(d, update=True)\n",
    "model.train(d, total_examples=len(d), epochs=20)\n",
    "model.save(\"ft_chars_twitter_finetuned_20_allchars.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://radimrehurek.com/gensim/models/fasttext.html#gensim.models.fasttext.FastText"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
