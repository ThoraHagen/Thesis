{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_serving.client import BertClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense, Dropout, LSTM, BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Command line expressions for regular BERT and fine-tuned BERT alternatively. Adjust paths as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bert-serving-start -pooling_strategy NONE -max_seq_len 100 -model_dir G:/uncased_L-12_H-768_A-12 \n",
    "#bert-serving-start -pooling_strategy NONE -max_seq_len 100 -model_dir G:/uncased_L-12_H-768_A-12 -tuned_model_dir G:/model_checkpoint  -ckpt_name=model.ckpt-2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To save time, only the dev set was used at some point. Testing on train and test set yielded the same results.\n",
    "path_dev_tsv='../Data_preprocessed/dev.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def get_labels(path):\n",
    "    df = pd.read_csv(path, sep='\\t', index_col=0)\n",
    "    labels = list(df['label'])\n",
    "    print(Counter(labels))\n",
    "    labels = np.asarray(labels)\n",
    "    labels = to_categorical(labels)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts(path):\n",
    "    df = pd.read_csv(path, sep='\\t', index_col=0)\n",
    "    texts = list(df['text'])\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = get_texts(path_dev_tsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['add me back you twat',\n",
       " 'ruffled ntac eileen dahlia - beautiful color combination of pink orange yellow white a coll httptcoHdYEBvnZB',\n",
       " \"i know bro nigga seen a ' african playing guard and was handling the hoe\",\n",
       " 'w left in the game are you retarded',\n",
       " \"none of us went to these prestigious establishments so we're clearly not as superior as these fucking retards\",\n",
       " \"i called at least times to make my hair appointment and these bitches don't pick - - guess i have to stop in to make an appointment grrr\",\n",
       " 'i also like flip a bitch which is gabbys qualitywords',\n",
       " 'why is that most of the niggas bitches i use to fw like to sub me face with tears of joy loudly crying face',\n",
       " 'eee u sick fuck what do u even do oo politics you fuckin odd cunt',\n",
       " 'you i was like you sneaky shit i saw him before i saw your dm because my phone is retarded']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = BertClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts[1648] = '-' #tweet is empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\thora\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\bert_serving\\client\\__init__.py:294: UserWarning: \"show_tokens=True\", but the server does not support showing tokenization info to clients.\n",
      "here is what you can do:\n",
      "- start a new server with \"bert-serving-start -show_tokens_to_client ...\"\n",
      "- or, use \"encode(show_tokens=False)\"\n",
      "  warnings.warn('\"show_tokens=True\", but the server does not support showing tokenization info to clients.\\n'\n"
     ]
    }
   ],
   "source": [
    "vecs = bc.encode(texts, show_tokens=True, is_tokenized=False) #is_tokenized=False, as many words were not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3000, 100, 768)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({0: 1000, 2: 1000, 1: 1000})\n"
     ]
    }
   ],
   "source": [
    "labels = get_labels(path_dev_tsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100, 768)          0         \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 5)                 15480     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 5)                 0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 3)                 18        \n",
      "=================================================================\n",
      "Total params: 15,498\n",
      "Trainable params: 15,498\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import Input, layers, Model\n",
    "input_tensor= Input(shape=(100,768))\n",
    "\n",
    "x = layers.LSTM(5)(input_tensor)\n",
    "x=layers.Dropout(0.5)(x)\n",
    "output_tensor=layers.Dense(3, activation='softmax')(x)\n",
    "\n",
    "model2 = Model(input_tensor, output_tensor)\n",
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2400 samples, validate on 600 samples\n",
      "Epoch 1/10\n",
      "2400/2400 [==============================] - 5s 2ms/step - loss: 1.0989 - acc: 0.3292 - val_loss: 1.0987 - val_acc: 0.3283\n",
      "Epoch 2/10\n",
      "2400/2400 [==============================] - 5s 2ms/step - loss: 1.0986 - acc: 0.3392 - val_loss: 1.0987 - val_acc: 0.3283\n",
      "Epoch 3/10\n",
      "2400/2400 [==============================] - 5s 2ms/step - loss: 1.0990 - acc: 0.3379 - val_loss: 1.0988 - val_acc: 0.3283\n",
      "Epoch 4/10\n",
      "2400/2400 [==============================] - 5s 2ms/step - loss: 1.0986 - acc: 0.3454 - val_loss: 1.0986 - val_acc: 0.3283\n",
      "Epoch 5/10\n",
      "2400/2400 [==============================] - 5s 2ms/step - loss: 1.0989 - acc: 0.3296 - val_loss: 1.0986 - val_acc: 0.3283\n",
      "Epoch 6/10\n",
      "2400/2400 [==============================] - 5s 2ms/step - loss: 1.0991 - acc: 0.3271 - val_loss: 1.0987 - val_acc: 0.3317\n",
      "Epoch 7/10\n",
      "2400/2400 [==============================] - 5s 2ms/step - loss: 1.0987 - acc: 0.3354 - val_loss: 1.0986 - val_acc: 0.3400\n",
      "Epoch 8/10\n",
      "2400/2400 [==============================] - 5s 2ms/step - loss: 1.0987 - acc: 0.3383 - val_loss: 1.0988 - val_acc: 0.3283\n",
      "Epoch 9/10\n",
      "2400/2400 [==============================] - 4s 2ms/step - loss: 1.0990 - acc: 0.3267 - val_loss: 1.0988 - val_acc: 0.3283\n",
      "Epoch 10/10\n",
      "2400/2400 [==============================] - 4s 2ms/step - loss: 1.0990 - acc: 0.3154 - val_loss: 1.0987 - val_acc: 0.3317\n"
     ]
    }
   ],
   "source": [
    "model2.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "history=model2.fit(vecs, labels,\n",
    "                            epochs=10,\n",
    "                            validation_split=0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
